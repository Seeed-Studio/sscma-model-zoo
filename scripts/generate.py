import os
import json
import requests
import subprocess
import openai
import nbformat as nbf
from tabulate import tabulate

def find_files_in_folder(folder_path, extension, exclude=[]):
    files = []
    for root, _, filenames in os.walk(folder_path):
        if root in exclude:
            continue
        for file in filenames:
            if file.endswith(f".{extension}"):
                files.append(os.path.join(root, file))
    return files

def get_current_commit_hash():
    try:
        result = subprocess.check_output(['git', 'rev-parse', 'HEAD'])
        commit_hash = result.decode().strip()
        return commit_hash
    except subprocess.CalledProcessError:
        return None

def get_current_branch():
    try:
        result = subprocess.check_output(['git', 'rev-parse', '--abbrev-ref', 'HEAD'])
        branch = result.decode().strip()
        return branch
    except subprocess.CalledProcessError:
        return None

def is_link_valid(url):
    try:
        r = requests.head(url)
        return r.status_code == 200
    except:
        return False

def check_json(model):
    
    check_list = ["name", "algorithm", "version", "category", "config", "dataset", "classes", "image", "description", "network", "benchmark"]
    for item in check_list:
        if item not in model:
            raise ValueError("Missing key: {}".format(item))
    
    for item in model["benchmark"]:
        if is_link_valid(item["url"]) == False:
            raise ValueError("Invalid link: {}".format(item["url"]))

def generate_doc_zh_CN(model):
    
    branch = get_current_branch()
    object_name = "{}_{}_{}".format(model["name"].replace(' ', '_'), model["algorithm"].replace(' ', '_'), model["network"]["input"]["shape"][0])
    
    doc = ""
    
    # Ê∑ªÂä†Ê®°ÂûãÂêçÁß∞ÂíåÊèèËø∞
    doc += "# {} - {}\n\n".format(model["name"], model["algorithm"])
    
    # Add en
    doc += "[English](../en/{}.md) | ÁÆÄ‰Ωì‰∏≠Êñá ".format(object_name)
    
    # Ê∑ªÂä†ColabÂæΩÁ´†
    doc += "[![Âú®Colab‰∏≠ÊâìÂºÄ](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seeed-studio/sscma-model-zoo/blob/{}/notebooks/zh_CN/{}.ipynb)\n\n".format(branch, object_name)


    # Ê∑ªÂä†Ê®°ÂûãÂæΩÁ´†
    doc += "**ÁâàÊú¨Ôºö** {}\n\n".format(model["version"])
    doc += "**Á±ªÂà´Ôºö** {}\n\n".format(model["category"])
    doc += "**ÁÆóÊ≥ïÔºö** [{}]({})\n\n".format(model["algorithm"], model["config"]["url"])
    doc += "**Êï∞ÊçÆÈõÜÔºö** [{}]({})\n\n".format(model["dataset"]["name"], model["dataset"]["url"])

    # Ê∑ªÂä†Á±ªÂà´
    doc += "**Á±ªÂà´Ôºö** "
    doc += ", ".join(["`{}`".format(c) for c in model["classes"]]) + "\n\n"

    # Ê∑ªÂä†Ê®°ÂûãÂõæÁâá
    doc += "![{}]({})\n\n".format(model["name"], model["image"])
        
    # Ê∑ªÂä†Ê®°ÂûãÊèèËø∞
    doc += model["description"] + "\n\n"
    
    # Ê∑ªÂä†ÁΩëÁªúÊû∂ÊûÑ
    doc += "### ÁΩëÁªúÊû∂ÊûÑ\n\n"
    network = model["network"]
    network_headers = ["", "Á±ªÂûã", "ÊâπÊ¨°", "ÂΩ¢Áä∂", "Â§áÊ≥®"]
    network_table = []
    network_table.append(["ËæìÂÖ•", network["input"]["type"], network["batch"], network["input"]["shape"], network["input"].get("remark", "")])   
    network_table.append(["ËæìÂá∫", network["output"]["type"], network["batch"], network["output"]["shape"], network["output"].get("remark", "")])
    doc += tabulate(network_table, network_headers, tablefmt="pipe", numalign="center", floatfmt=".2f") + "\n"
        
    # Ê∑ªÂä†Âü∫ÂáÜÊµãËØï
    doc += "### Âü∫ÂáÜÊµãËØï\n\n"
    benchmark_headers = ["Ê°ÜÊû∂", "Á≤æÂ∫¶"]
    benchmark_table = []
    metrics_headers = []
    inference = []
        
    benchmarks = model["benchmark"]
    for benchmark in benchmarks:
        metrics = benchmark.get("metrics", {})
        for key, value in metrics.items():
            if key == "Inference(ms)":
                for k, v in value.items():
                    if k not in inference:
                        inference.append(k)
            if key not in metrics_headers:
                metrics_headers.append(key)

    for key in metrics_headers:
        benchmark_headers.append(key)
        
    benchmark_headers.append("‰∏ãËΩΩ")
    benchmark_headers.append("‰ΩúËÄÖ")
        
    for benchmark in benchmarks:
        backend = benchmark.get("backend", "")
        precision = benchmark.get("precision", "")
        metrics = benchmark.get("metrics", {})
        link = "[ÈìæÊé•]({})".format(benchmark.get("url", ""))
        author = benchmark.get("author", "")
        benchmark_table.append([backend, precision])
        for key in metrics_headers:
            if key in metrics:
                if key == "Inference(ms)":
                    benchmark_table[-1].append("<br>".join([str("{}<sup>({})</sup>".format(metrics[key].get(k, "-"), inference.index(k)+1)) for k in inference]))
                else:
                    benchmark_table[-1].append(metrics[key])
            else:
                benchmark_table[-1].append("-") 
                    
        benchmark_table[-1].append(link)
        benchmark_table[-1].append(author)
            
    doc += tabulate(benchmark_table, benchmark_headers, tablefmt="pipe", numalign="center", stralign="center", floatfmt=".2f") + "\n"
        
    # Ê∑ªÂä†Ë°®Ê†ºÊ≥®Èáä
    doc += "\n"
    doc += "***Ë°®Ê†ºÊ≥®ÈáäÔºö***\n\n"
    if "benchmark_note" in model:
        for key, value in model["benchmark_note"].items():
            doc += "- ***{}Ôºö*** {}.*\n".format(key, value)
    doc += "- ***Ê°ÜÊû∂Ôºö** Áî®‰∫éÊé®Êñ≠Ê®°ÂûãÁöÑÊ∑±Â∫¶Â≠¶‰π†Ê°ÜÊû∂.*\n"
    doc += "- ***Á≤æÂ∫¶Ôºö** Áî®‰∫éËÆ≠ÁªÉÊ®°ÂûãÁöÑÊï∞ÂÄºÁ≤æÂ∫¶.*\n"
    doc += "- ***ÊåáÊ†áÔºö** Áî®‰∫éËØÑ‰º∞Ê®°ÂûãÁöÑÊåáÊ†á.*\n"
    doc += "- ***Êé®ÁêÜÊó∂Èó¥ÔºàÊØ´ÁßíÔºâÔºö** Ê®°ÂûãÁöÑÊé®ÁêÜÊó∂Èó¥Ôºà‰ª•ÊØ´Áßí‰∏∫Âçï‰ΩçÔºâ.*\n"
    for i, key in enumerate(inference):
        doc += "  - ***{}Ôºö** {}.*\n".format(i+1, key)
    doc += "- ***ÈìæÊé•Ôºö** Ê®°ÂûãÁöÑÈìæÊé•.*\n"
    doc += "- ***‰ΩúËÄÖÔºö** Ê®°ÂûãÁöÑ‰ΩúËÄÖ.*\n"
    doc += "\n"
        
    # Ê∑ªÂä†‰ΩøÁî®ÊåáÂçó
    doc += "## ‰ΩøÁî®ÊåáÂçó\n\n"
    if model.get("guidelines", "") != "":
        doc += model.get("guidelines", "")
        
    # Ê∑ªÂä†ËÆ∏ÂèØËØÅ
    doc += "### ËÆ∏ÂèØËØÅ\n\n"
    doc += model.get("license", "")
    doc += "\n\n"
        
    return doc

def generate_doc_en(model):
    
    branch = get_current_branch()
    object_name = "{}_{}_{}".format(model["name"].replace(' ', '_'), model["algorithm"].replace(' ', '_'), model["network"]["input"]["shape"][0])
    
    doc = ""

    
    # Add model name and description
    doc += "# {} - {}\n\n".format(model["name"], model["algorithm"])
    
    # Add zh_CN
    doc += "English | [ÁÆÄ‰Ωì‰∏≠Êñá](../zh_CN/{}.md) ".format(object_name)
    
    # Add Colab Badge
    doc += "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seeed-studio/sscma-model-zoo/blob/{}/notebooks/en/{}.ipynb)\n\n".format(branch, object_name)


    # Add model badges
    doc += "**Version:** {}\n\n".format(model["version"])
    doc += "**Category:** {}\n\n".format(model["category"])
    doc += "**Algorithm:** [{}]({})\n\n".format(model["algorithm"], model["config"]["url"])
    doc += "**Dataset:** [{}]({})\n\n".format(model["dataset"]["name"], model["dataset"]["url"])

    # Add class
    doc += "**Class:** "
    doc += ", ".join(["`{}`".format(c) for c in model["classes"]]) + "\n\n"

    # Add model image
    doc += "![{}]({})\n\n".format(model["name"], model["image"])
        
    #Add model description
    doc += model["description"] + "\n\n"
    
    # Add Network Architecture
    doc += "### Network \n\n"
    network = model["network"]
    network_headers = ["", "Type", "Batch", "Shape", "Remark"]
    network_table = []
    network_table.append(["Input", network["input"]["type"], network["batch"], network["input"]["shape"], network["input"].get("remark", "")])   
    network_table.append(["Output", network["output"]["type"], network["batch"], network["output"]["shape"], network["output"].get("remark", "")])
    doc += tabulate(network_table, network_headers, tablefmt="pipe", numalign="center", floatfmt=".2f") + "\n"
        
    # Add Benchmark benchmark_table
    doc += "### Benchmark\n\n"
    benchmark_headers = ["Backend", "Precision"]
    benchmark_table = []
    metrics_headers = []
    inference = []
        
    benchmarks = model["benchmark"]
    for benchmark in benchmarks:
        metrics = benchmark.get("metrics", {})
        for key, value in metrics.items():
            if key == "Inference(ms)":
                for k, v in value.items():
                    if k not in inference:
                        inference.append(k)
            if key not in metrics_headers:
                metrics_headers.append(key)

    for key in metrics_headers:
        benchmark_headers.append(key)
        
    benchmark_headers.append("Download")
    benchmark_headers.append("Author")
        
    for benchmark in benchmarks:
        backend = benchmark.get("backend", "")
        precision = benchmark.get("precision", "")
        metrics = benchmark.get("metrics", {})
        link = "[Link]({})".format(benchmark.get("url", ""))
        author = benchmark.get("author", "")
        benchmark_table.append([backend, precision])
        for key in metrics_headers:
            if key in metrics:
                if key == "Inference(ms)":
                    benchmark_table[-1].append("<br>".join([str("{}<sup>({})</sup>".format(metrics[key].get(k, "-"), inference.index(k)+1)) for k in inference]))
                else:
                    benchmark_table[-1].append(metrics[key])
            else:
                benchmark_table[-1].append("-") 
                    
        benchmark_table[-1].append(link)
        benchmark_table[-1].append(author)
            
    doc += tabulate(benchmark_table, benchmark_headers, tablefmt="pipe", numalign="center", stralign="center", floatfmt=".2f") + "\n"
        
    # Add Table Notes
    doc += "\n"
    doc += "***Table Notes:***\n\n"
    if "benchmark_note" in model:
        for key, value in model["benchmark_note"].items():
            doc += "- ***{}:** {}.*\n".format(key, value)
    doc += "- ***Backend:** The deep learning framework used to infer the model.*\n"
    doc += "- ***Precision:** The numerical precision used for training the model.*\n"
    doc += "- ***Metrics:** The metrics used to evaluate the model.*\n"
    doc += "- ***Inference(ms):** The inference time of the model in milliseconds.*\n"
    for i, key in enumerate(inference):
        doc += "  - ***{}:** {}.*\n".format(i+1, key)
    doc += "- ***Link:** The link to the model.*\n"
    doc += "- ***Author:** The author of the model.*\n"
    doc += "\n"
        
    # Add Guidelines
    if model.get("guidelines", "") != "":
        doc += "## Guidelines\n\n"
        doc += model.get("guidelines", "")
        
    # Add License
    doc += "### License\n\n"
    doc += model.get("license", "")
    doc += "\n\n"
        
    return doc

def generate_notebook_en(model):
    work_dir = os.getcwd()
    object_name = "{}_{}_{}".format(model["name"].replace(' ', '_'), model["algorithm"].replace(' ', '_'), model["network"]["input"]["shape"][0])
    file = open(os.path.join(work_dir, "templates", "notebook.template.en.ipynb"))
    config = model.get("config", "")
    config_file = config.get("url", "config.py")
    config_name = os.path.basename(config_file).split(".")[0]
    config_path = 'configs/custom/{}'.format(config_file.split("/")[-1])
    benchmark = [item for item in model.get("benchmark", {}) if item.get('backend') == "PyTorch"]
    pretrain_url = benchmark[0].get("url", "")
    pretrain_path = 'pretrains/{}'.format(pretrain_url.split("/")[-1])
    dataset_url = model.get("dataset", {}).get("download", "")
    if file is None:
        raise ValueError("Invalid notebook template file")
    
    notebook = nbf.read(file, as_version=4)
    
    # Add model name and description
    content = ""
    content += "## üìï{} - {}\n\n".format(model["name"], model["algorithm"])

    # Add model badges
    content += "**Version:** {}\n\n".format(model["version"])
    content += "**Category:** {}\n\n".format(model["category"])
    content += "**Algorithm:** [{}]({})\n\n".format(model["algorithm"], model["config"]["url"])
    content += "**Dataset:** [{}]({})\n\n".format(model["dataset"]["name"], model["dataset"]["url"])

    # Add class
    content += "**Class:** "
    content += ", ".join(["`{}`".format(c) for c in model["classes"]]) + "\n\n"

    # Add model image
    content += "![{}]({})\n\n".format(model["name"], model["image"])
        
    # Add model description
    content += model["description"] + "\n\n"
    
    notebook['cells'][1]['source'] = content
    
    # Prepare Config
    notebook['cells'][5]['source'] = '%mkdir -p configs/custom \n'
    notebook['cells'][5]['source'] += '!wget -c {} -O {}'.format(config.get("url", ""), config_path)
    
    # Prepare Pretrain
    notebook['cells'][7]['source'] = '%mkdir -p pretrains \n'
    notebook['cells'][7]['source'] += '!wget -c {} -O {}'.format(pretrain_url, pretrain_path)
    
    # Prepare Dataset
    notebook['cells'][9]['source'] = '%mkdir -p datasets \n'
    if dataset_url != "":
        notebook['cells'][9]['source'] += '!wget -c {} -O {} \n'.format(dataset_url, "data.zip")
        notebook['cells'][9]['source'] += '!unzip -q datasets/data.zip -d datasets/data'.format()
    else:
        notebook['cells'][9]['source'] += "# Auto Fetch By SSCMA"

    # Train Model 
    if config != "":
        notebook['cells'][11]['source'] = '''!sscma.train {} \\
--cfg-options  {} \\
\tnum_classes={} \\
\tepoch=10 '''.format(config_path, config.get("argument", ""), len(model["classes"]))
        if dataset_url != "":
            notebook['cells'][11]['source'] += " \\\n\tdata_root=datasets/data/"
        
        # Export Model
        notebook['cells'][13]['source'] = "import os\n"
        notebook['cells'][13]['source'] += "with open('work_dirs/{}/last_checkpoint', 'w') as f:\n\tos.environ['CHECKPOINT_FILE_PATH'] = f.read()".format(config_name)
        notebook['cells'][14]['source'] = '''!sscma.export {} $CHECKPOINT_FILE_PATH \\
--cfg-options {} \\
\tnum_classes={} \\
\tepoch=10'''.format(config_path, config_name, config.get("argument", ""), len(model["classes"]))
        if dataset_url != "":
            notebook['cells'][14]['source'] += " \\\n\tdata_root=datasets/data/"
        # Inference Model
        notebook['cells'][17]['source'] = '''!sscma.infernce {}  $CHECKPOINT_FILE_PATH \\
--cfg-options {} \\
\tnum_classes={} \\
\tepoch=10'''.format(config_path, config.get("argument", ""), len(model["classes"]))
        if dataset_url != "":
                    notebook['cells'][17]['source'] += " \\\n\tdata_root=datasets/data/"
        
        notebook['cells'][19]['source'] = '''!sscma.infernce {} ${{CHECKPOINT_FILE_PATH%.*}}_float32.onnx \\
--cfg-options {} \\
\tnum_classes={} \\
\tepoch=10'''.format(config_path, config.get("argument", ""), len(model["classes"]))
        if dataset_url != "":
            notebook['cells'][19]['source'] += " \\\n\tdata_root=datasets/data/"

        notebook['cells'][21]['source'] = '''!sscma.infernce {} ${{CHECKPOINT_FILE_PATH%.*}}_float32.tflite \\
--cfg-options {} \\
\tnum_classes={} \\
\tepoch=10'''.format(config_path, config.get("argument", ""), len(model["classes"]))
        if dataset_url != "":
            notebook['cells'][21]['source'] += " \\\n\tdata_root=datasets/data/"
     
        notebook['cells'][23]['source'] = '''!sscma.infernce {} ${{CHECKPOINT_FILE_PATH%.*}}_int8.tfite \\
--cfg-options {} \\
\tnum_classes={} \\
\tepoch=10'''.format(config_path, config.get("argument", ""), len(model["classes"]))
        if dataset_url != "":
            notebook['cells'][23]['source'] += " \\\n\tdata_root=datasets/data/"
    
    return notebook



def generate_notebook_zh_CN(model):
    work_dir = os.getcwd()
    object_name = "{}_{}_{}".format(model["name"].replace(' ', '_'), model["algorithm"].replace(' ', '_'), model["network"]["input"]["shape"][0])
    file = open(os.path.join(work_dir, "templates", "notebook.template.zh_CN.ipynb"))
    config = model.get("config", "")
    config_file = config.get("url", "config.py")
    config_name = os.path.basename(config_file).split(".")[0]
    config_path = 'configs/custom/{}'.format(config_file.split("/")[-1])
    benchmark = [item for item in model.get("benchmark", {}) if item.get('backend') == "PyTorch"]
    pretrain_url = benchmark[0].get("url", "")
    pretrain_path = 'pretrains/{}'.format(pretrain_url.split("/")[-1])
    dataset_url = model.get("dataset", {}).get("download", "")
    if file is None:
        raise ValueError("Invalid notebook template file")
    
    notebook = nbf.read(file, as_version=4)
    
    # Add model name and description
    content = ""
    content += "## üìï{} - {}\n\n".format(model["name"], model["algorithm"])

    # Add model badges
    content += "**ÁâàÊú¨:** {}\n\n".format(model["version"])
    content += "**Á±ªÂà´:** {}\n\n".format(model["category"])
    content += "**ÁÆóÊ≥ï:** [{}]({})\n\n".format(model["algorithm"], model["config"]["url"])
    content += "**Êï∞ÊçÆÈõÜ:** [{}]({})\n\n".format(model["dataset"]["name"], model["dataset"]["url"])

    # Add class
    content += "**Class:** "
    content += ", ".join(["`{}`".format(c) for c in model["classes"]]) + "\n\n"

    # Add model image
    content += "![{}]({})\n\n".format(model["name"], model["image"])
        
    # Add model description
    content += model["description"] + "\n\n"
    
    notebook['cells'][1]['source'] = content
    
    # Prepare Config
    notebook['cells'][5]['source'] = '%mkdir -p configs/custom \n'
    notebook['cells'][5]['source'] += '!wget -c {} -O {}'.format(config.get("url", ""), config_path)
    
    # Prepare Pretrain
    notebook['cells'][7]['source'] = '%mkdir -p pretrains \n'
    notebook['cells'][7]['source'] += '!wget -c {} -O {}'.format(pretrain_url, pretrain_path)
    
    # Prepare Dataset
    notebook['cells'][9]['source'] = '%mkdir -p datasets \n'
    if dataset_url != "":
        notebook['cells'][9]['source'] += '!wget -c {} -O {} \n'.format(dataset_url, "data.zip")
        notebook['cells'][9]['source'] += '!unzip -q datasets/data.zip -d datasets/data'.format()
    else:
        notebook['cells'][9]['source'] += "# Auto Fetch By SSCMA"
        
    # Train Model 
    if config != "":
        notebook['cells'][11]['source'] = '''!sscma.train {} \\
--cfg-options  {} \\
\tnum_classes={} \\
\tepoch=10 '''.format(config_path, config.get("argument", ""), len(model["classes"]))
        if dataset_url != "":
            notebook['cells'][11]['source'] += " \\\n\tdata_root=datasets/data/"
        
        # Export Model
        notebook['cells'][13]['source'] = "import os\n"
        notebook['cells'][13]['source'] += "with open('work_dirs/{}/last_checkpoint', 'w') as f:\n\tos.environ['CHECKPOINT_FILE_PATH'] = f.read()".format(config_name)
        notebook['cells'][14]['source'] = '''!sscma.export {} $CHECKPOINT_FILE_PATH \\
--cfg-options {} \\
\tnum_classes={} \\
\tepoch=10'''.format(config_path, config_name, config.get("argument", ""), len(model["classes"]))
        if dataset_url != "":
            notebook['cells'][14]['source'] += " \\\n\tdata_root=datasets/data/"
        # Inference Model
        notebook['cells'][17]['source'] = '''!sscma.infernce {}  $CHECKPOINT_FILE_PATH \\
--cfg-options {} \\
\tnum_classes={} \\
\tepoch=10'''.format(config_path, config.get("argument", ""), len(model["classes"]))
        if dataset_url != "":
                    notebook['cells'][17]['source'] += " \\\n\tdata_root=datasets/data/"
        
        notebook['cells'][19]['source'] = '''!sscma.infernce {} ${{CHECKPOINT_FILE_PATH%.*}}_float32.onnx \\
--cfg-options {} \\
\tnum_classes={} \\
\tepoch=10'''.format(config_path, config.get("argument", ""), len(model["classes"]))
        if dataset_url != "":
            notebook['cells'][19]['source'] += " \\\n\tdata_root=datasets/data/"

        notebook['cells'][21]['source'] = '''!sscma.infernce {} ${{CHECKPOINT_FILE_PATH%.*}}_float32.tflite \\
--cfg-options {} \\
\tnum_classes={} \\
\tepoch=10'''.format(config_path, config.get("argument", ""), len(model["classes"]))
        if dataset_url != "":
            notebook['cells'][21]['source'] += " \\\n\tdata_root=datasets/data/"
     
        notebook['cells'][23]['source'] = '''!sscma.infernce {} ${{CHECKPOINT_FILE_PATH%.*}}_int8.tfite \\
--cfg-options {} \\
\tnum_classes={} \\
\tepoch=10'''.format(config_path, config.get("argument", ""), len(model["classes"]))
        if dataset_url != "":
            notebook['cells'][23]['source'] += " \\\n\tdata_root=datasets/data/"
                              
    # Export
    return notebook


def openai_reply(content, apikey):
    openai.api_key = apikey
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "You're an AI assistant who's good at translating English into Chinese."},
            {"role": "system", "content": 'Translate the following English nootbook to ChineseÔºö"{}"'.format(content)},
            ],
        temperature=0.5,
        max_tokens=2500,
        top_p=1,
        frequency_penalty=0,
        presence_penalty=0,
    )
    return response.choices[0].message.content

def main():
    
    branch = get_current_branch()
    work_dir = os.getcwd()
    dist_dir = os.path.join(work_dir)
    if not os.path.exists(dist_dir):
        os.makedirs(dist_dir)
        
    sscma_model_data = '{"version": " 1", "models":[]}'
    sscma_model_json = json.loads(sscma_model_data)
    sscma_model_json["version"] = get_current_commit_hash()
    
    for file in find_files_in_folder("./", "json", exclude=["./", "./dist", "./.github", "./docs", "./notebooks", "./scripts", "./templates"]):
        print("Processing {}".format(file))
        with open(file, "r") as f:
            str = f.read()
            model = json.loads(str)
            sscma_model_json["models"].append(model)
            # if check_json(model) == False:
            #     raise ValueError("Invalid models.json file - {}".format(file))
            doc_en_dir = os.path.join(dist_dir, "docs/en")
            if not os.path.exists(doc_en_dir):
                os.makedirs(doc_en_dir) 
                
            
            object_name = "{}_{}_{}".format(model["name"].replace(' ', '_'), model["algorithm"].replace(' ', '_'), model["network"]["input"]["shape"][0])
            #generate doc 
            doc_en_path = os.path.join(doc_en_dir, "{}.md".format(object_name))
            doc_en = generate_doc_en(model)
            if not os.path.exists(os.path.dirname(doc_en_path)):
                os.makedirs(os.path.dirname(doc_en_path))
            with open(os.path.join(doc_en_path), "w") as f:
                f.write(doc_en)
                
                
            doc_zh_CN_dir = os.path.join(dist_dir, "docs/zh_CN")
            if not os.path.exists(doc_zh_CN_dir):
                os.makedirs(doc_zh_CN_dir) 
                
            doc_zh_CN_path = os.path.join(doc_zh_CN_dir, "{}.md".format(object_name))
            doc_zh_CN = generate_doc_zh_CN(model)
            if not os.path.exists(os.path.dirname(doc_zh_CN_path)):
                os.makedirs(os.path.dirname(doc_zh_CN_path))
            with open(os.path.join(doc_zh_CN_path), "w") as f:
                f.write(doc_zh_CN)
            
            # generate notebook
            notebook_en_dir = os.path.join(dist_dir, "notebooks/en")
            if not os.path.exists(notebook_en_dir):
                os.makedirs(notebook_en_dir)
            notebook_en_path = os.path.join(notebook_en_dir, "{}.ipynb".format(object_name))
            notebook_en = generate_notebook_en(model)
            if not os.path.exists(os.path.dirname(notebook_en_path)):
                os.makedirs(os.path.dirname(notebook_en_path))
            with open(os.path.join(notebook_en_path), "w") as f:
                nbf.write(notebook_en, f)
                
            notebook_zh_CN_dir = os.path.join(dist_dir, "notebooks/zh_CN")
            if not os.path.exists(notebook_zh_CN_dir):
                os.makedirs(notebook_zh_CN_dir)
            notebook_zh_CN_path = os.path.join(notebook_zh_CN_dir, "{}.ipynb".format(object_name))
            notebook_zh_CN = generate_notebook_zh_CN(model)
            if not os.path.exists(os.path.dirname(notebook_zh_CN_path)):
                os.makedirs(os.path.dirname(notebook_zh_CN_path))
            with open(os.path.join(notebook_zh_CN_path), "w") as f:
                nbf.write(notebook_zh_CN, f)
            
    
    with open(os.path.join(dist_dir, "models.json"), "w") as f:
        json.dump(sscma_model_json, f, indent=4)
        
    # generate README.md
    model_list_en = ""
    model_list_zh_CN = ""
    
    categories = []
    for model in sscma_model_json["models"]:
        if model["category"] not in categories:
            categories.append(model["category"])
    
    for category in categories:
        model_list_en += "### {}\n\n".format(category)
        header_en = ["Model", "Colab"]
        table_en = []
        model_list_zh_CN += "### {}\n\n".format(category)
        header_zh_CN = ["Ê®°Âûã", "Colab"]
        table_zh_CN = []
        for model in sscma_model_json["models"]:
            if model["category"] == category:
                object_name = "{}_{}_{}".format(model["name"].replace(' ', '_'), model["algorithm"].replace(' ', '_'), model["network"]["input"]["shape"][0])
                table_en.append(["[{}]({})".format(object_name, "docs/en/{}.md".format(object_name)), "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seeed-studio/sscma-model-zoo/blob/{}/notebooks/en/{}.ipynb)".format(branch,object_name)])
                table_zh_CN.append(["[{}]({})".format(object_name, "docs/zh_CN/{}.md".format(object_name)), "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seeed-studio/sscma-model-zoo/blob/{}/notebooks/zh_CN/{}.ipynb)".format(branch,object_name)])
        model_list_en += tabulate(table_en, header_en, tablefmt="pipe", numalign="center", stralign="left") + "\n\n"
        model_list_zh_CN += tabulate(table_zh_CN, header_zh_CN, tablefmt="pipe", numalign="center", stralign="left") + "\n\n"
    
    with open(os.path.join(work_dir, "templates", "README.template.en.md"), "r") as file:
        readme_en = file.read()
    
        readme_en = readme_en.replace("{{model_list}}", model_list_en)
    
        with open(os.path.join(dist_dir, "README.md"), "w") as f:
            f.write(readme_en)
            
    
    with open(os.path.join(work_dir, "templates", "README.template.zh_CN.md"), "r") as file:
        readme_zh_CN = file.read()
        print()
        readme_zh_CN = readme_zh_CN.replace("{{model_list}}", model_list_zh_CN)
    
        with open(os.path.join(dist_dir, "README_zh_CN.md"), "w") as f:
            f.write(readme_zh_CN)
    
    
if __name__ == "__main__":
    main()